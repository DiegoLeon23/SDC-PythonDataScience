{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://www.mindinventory.com/blog/wp-content/uploads/2019/04/python-development-1200x500.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programa de Especialización en Python\n",
    "\n",
    "### Manuel Sigüeñas, M.Sc.(c)\n",
    "### Prof. Lenguajes de Programación para Ciencia de Datos / Agile Data Scientists / SCRUMStudy Certified Trainer\n",
    "[msiguenas@socialdata-peru.com](msiguenas@socialdata-peru.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de los datos de revisión de películas de IMDb para el procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención del conjunto de datos de revisión de películas de IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install PyPrind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento del dataset de película en un formato más conveniente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############                ] 100% | ETA: 00:01:46"
     ]
    }
   ],
   "source": [
    "import pyprind ### paquete para contabilizar el proceso de esperar \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# cambiar la 'basepath' al directorio de la\n",
    "# Conjunto de datos de película descomprimido\n",
    "\n",
    "basepath = 'D:/Python/3. Nivel III/7-8/aclImdb_v1.tar/aclImdb'\n",
    "## En el archivo se encuentra separados por carpetas, los comentarios de 50000 personas en archivo txt. \n",
    "## Tienes 2 carpetas que se dividen en train y en test, cada una de ellas tiene dos sub carpetas, pos y neg\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0} ## asignacion de una variables, si los comentarios estan en la carpeta \"pos\" es uno, sino es 0 \n",
    "pbar = pyprind.ProgBar(50000) ## Número de archivos a cargar.\n",
    "df = pd.DataFrame() ## Asignamos la función DataFrame a un objeto. \n",
    "for s in ('test', 'train'): ## Creamos un bucle, cuando s puede ser \"test\" o \" train\" \n",
    "    for l in ('pos', 'neg'): ## creamos un bucle dentro de otro, cuando l es \"pos\" o \"neg\" \n",
    "        path = os.path.join(basepath, s, l) ## asignamos al objeto \"path\" las rutas que pueden tomar, en total son 4. \n",
    "        for file in os.listdir(path): ## asignamos otra variable \"file\" que tendra la lista de todos los archivos text de cada ruta\n",
    "            with open(os.path.join(path, file), ## abrimos los 50000 archivos\n",
    "                      'r', encoding='utf-8') as infile: ## asignamos que el tipo de texto es \"uft-8\"\n",
    "                txt = infile.read() #creamos un objeto donde se alamcenara todo lo extraido\n",
    "            df = df.append([[txt, labels[l]]],  ## creamos un data frame donde asignaremos dos variables, los comentarios extraidos, y de donde proviene, si es negativo o positivo\n",
    "                           ignore_index=True)\n",
    "            pbar.update() \n",
    "df.columns = ['review', 'sentiment'] ## asignamos el nombre de las dos nuevas columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barajeando el DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np ## cambiamos el orden\n",
    "## establecemos una semilla\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index)) ## barajeamos el data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opcional: Guardar los datos ensamblados como archivo CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8') ## asignamos un nombre al nuevo data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this movie was absolutely hilarious....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pluses: Mary Boland is delightfully on edge as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is not very bad tjough. But one can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I thought this movie was absolutely hilarious....          1\n",
       "1  Pluses: Mary Boland is delightfully on edge as...          0\n",
       "2  This movie is not very bad tjough. But one can...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8') ## abrimos el archivo y lo guardamos en el objeto df\n",
    "df.head(3) ## observamos los 3 primeros textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de < bolsa de palabras \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando documentos en vectores de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al llamar al método fit_transform en CountVectorizer, acabamos de construir el vocabulario del modelo de bolsa de palabras y transformamos las tres frases siguientes en vectores de características dispersos:\n",
    "<br>\n",
    "\n",
    "1. El sol brilla\n",
    "2. El clima es dulce\n",
    "3. El sol está brillando, el clima es dulce, y uno y uno es dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer ## llamar a la función  \"feature_extraction.text\" para vectorizar(separar) las palabras \n",
    "\n",
    "count = CountVectorizer() #asignamos la función a un objeto \n",
    "docs = np.array([ ## creamos una lista \n",
    "        'El sol brilla',\n",
    "        'El clima es dulce',\n",
    "        'El sol está brillando, el clima es dulce, y uno y uno es dos'])\n",
    "bag = count.fit_transform(docs) ## vectorizamos y contamos las palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a imprimir el contenido del vocabulario para obtener una mejor comprensión de los conceptos subyacentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'el': 5, 'sol': 8, 'brilla': 0, 'clima': 2, 'es': 6, 'dulce': 4, 'está': 7, 'brillando': 1, 'uno': 9, 'dos': 3}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver al ejecutar el comando anterior, el vocabulario se almacena en un diccionario de Python, que asigna las palabras únicas que se asignan a índices enteros. A continuación vamos a imprimir los vectores de características que acabamos de crear:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada posición de índice en los vectores de entidad que se muestran aquí corresponde a los valores enteros que se almacenan como elementos de diccionario en el vocabulario CountVectorizer. Por ejemplo, la primera entidad en la posición de índice 0 se asemeja al recuento de la palabra y, que solo se produce en el último documento, y la palabra está en la posición de índice 1 (la 2a entidad de los vectores de documento) se produce en las tres oraciones. Esos valores en los vectores de entidad también se denominan frecuencias de términos sin procesar: *tf (t,d)*—el número de veces que se produce un término t en un documento *d*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 1 0 0 1 0]\n",
      " [0 0 1 0 1 1 1 0 0 0]\n",
      " [0 1 1 1 1 2 2 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de la relevancia de la palabra a través de la frecuencia de los documentos inversos en frecuencia de términos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando analizamos datos de texto, a menudo encontramos palabras que se producen en varios documentos de ambas clases. Las palabras que ocurren con frecuencia normalmente no contienen información útil o discriminatoria. En esta subsección, aprenderemos acerca de una técnica útil llamada frecuencia de documentos inversa de frecuencia de término (tf-idf) que se puede utilizar para reducir el peso de las palabras que ocurren con frecuencia en los vectores de entidades. El tf-idf se puede den como el producto del término frecuencia y la frecuencia inversa del documento:\n",
    "\n",
    "$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n",
    "\n",
    "Aquí el tf(t, d) es el término frecuencia que introdujimos en la sección anterior,\n",
    "y la frecuencia inversa del documento *idf(t, d)* se puede calcular como: \n",
    "\n",
    "$$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n",
    "\n",
    "donde $n_d$ es el número total de documentos, y *df(d, t)* es el número de documentos *d* que contienen el término *t*. Tenga en cuenta que agregar la constante 1 al denominador es opcional y sirve para asignar un valor distinto de cero a los términos que se producen en todas las muestras de entrenamiento; el registro se utiliza para asegurarse de que las frecuencias de documento bajas no reciben demasiado peso.\n",
    "\n",
    "Scikit-learn implementa otro transformador, el 'TfidfTransformer', que toma el término sin procesar frecuencias de 'CountVectorizer' como entrada y las transforma en tf-idfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.72 0.   0.   0.   0.   0.43 0.   0.   0.55 0.  ]\n",
      " [0.   0.   0.53 0.   0.53 0.41 0.53 0.   0.   0.  ]\n",
      " [0.   0.28 0.22 0.28 0.22 0.33 0.43 0.28 0.22 0.57]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, \n",
    "                         norm='l2', \n",
    "                         smooth_idf=True)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs))\n",
    "      .toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos en la subsección anterior, la palabra tiene la mayor frecuencia de términos en el 3er documento, siendo la palabra que ocurre con más frecuencia. Sin embargo, después de transformar el mismo vector de características en tf-idfs, vemos que la palabra es\n",
    "ahora asociado con un tf-idf relativamente pequeño (0,45) en el documento 3, ya que es\n",
    "documentos 1 y 2 y, por lo tanto, es poco probable que contenga información útil y discriminatoria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, si calcularamos manualmente los tf-idfs de los términos individuales en nuestros vectores de características, habríamos notado que el 'TfidfTransformer' calcula el tf-idfs ligeramente diferente en comparación con las ecuaciones de libros de texto estándar que demos anteriormente. Las ecuaciones para el idf y el tf-idf que se implementaron en scikit-learn son:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
    "\n",
    "La ecuación tf-idf que se implementó en scikit-learn es la siguiente:\n",
    "\n",
    "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
    "\n",
    "Aunque también es más típico normalizar las frecuencias de términos sin procesar antes de calcular el tf-idfs, el 'TfidfTransformer' normaliza directamente el tf-idfs.\n",
    "\n",
    "De forma predeterminada ('norm''l2''), TfidfTransformer de scikit-learn aplica la normalización L2, que devuelve un vector de longitud 1 dividiendo un vector de entidades no normalizado *v* por su norma L2:\n",
    "\n",
    "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
    "\n",
    "Para asegurarnos de que entendemos cómo funciona TfidfTransformer, caminemos\n",
    "a través de un ejemplo y calcular el tf-idf de la palabra está en el 3er documento.\n",
    "\n",
    "La palabra tiene una frecuencia de término de 3 (tf a 3) en el documento 3, y la frecuencia del documento de este término es 3 ya que el término se produce en los tres documentos (df 3). Por lo tanto, podemos calcular el idf de la siguiente manera:\n",
    "\n",
    "$$\\text{idf}(\"is\", d3) = log \\frac{1+3}{1+3} = 0$$\n",
    "\n",
    "Ahora, para calcular el tf-idf, simplemente necesitamos agregar 1 a la frecuencia inversa del documento y multiplicarlo por el término frecuencia:\n",
    "\n",
    "$$\\text{tf-idf}(\"is\",d3)= 3 \\times (0+1) = 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf of term \"is\" = 3.00\n"
     ]
    }
   ],
   "source": [
    "tf_is = 3\n",
    "n_docs = 3\n",
    "idf_is = np.log((n_docs+1) / (3+1))\n",
    "tfidf_is = tf_is * (idf_is + 1)\n",
    "print('tf-idf of term \"is\" = %.2f' % tfidf_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si repetimos estos cálculos para todos los términos del 3er documento, obtendríamos los siguientes vectores tf-idf: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]. Sin embargo, observamos que los valores de este vector de entidad son diferentes de los valores que obtuvimos del TfidfTransformer que usamos anteriormente. El paso nal que nos falta en este cálculo tf-idf es la l2-normalización, que se puede aplicar de la siguiente manera:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$\n",
    "\n",
    "$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n",
    "\n",
    "$$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, los resultados coinciden con los resultados devueltos por scikit-learn 'TfidfTransformer' (abajo). Puesto que ahora entendemos cómo se calculan los tf-idfs, procedamos a las siguientes secciones y apliquemos esos conceptos al conjunto de datos de revisión de películas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 1.69, 1.29, 1.69, 1.29, 2.  , 2.58, 1.69, 1.29, 3.39])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
    "raw_tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.28, 0.22, 0.28, 0.22, 0.33, 0.43, 0.28, 0.22, 0.57])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de datos de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'d you expect. <br /><br />Overall,just hilarious.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:] ## Localizamos un cometario del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re ## como se puede observar en el texto de arriba esta aun con parte del lenguaje html\n",
    "def preprocessor(text): ## definimos una función llamada preprocessor, que requerira al argumento \"text\"\n",
    "    text = re.sub('<[^>]*>', '', text)  ## el objeto llamado text se modificara para que elimine el código del lenguaje html\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', ## asignamos un nuevo ojeto que almacenara a los emoticones del archivo modificado\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + \n",
    "            ' '.join(emoticons).replace('-', ''))## el archivo text es modificado \n",
    "    return ## text es el obejto a retornar es los modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:]) ### probamos con una parte de la data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\") ## probamos la retención de los emoticones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor) ## aplicamos la función creada a la data entera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de documentos en tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):  ## función para tpkenizar los datos\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text): ## función para tokenizar los datos y llevarlos a su raiz\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'los',\n",
       " 'corredores',\n",
       " 'les',\n",
       " 'gusta',\n",
       " 'correr',\n",
       " 'y',\n",
       " 'por',\n",
       " 'lo',\n",
       " 'tanto',\n",
       " 'corren']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('a los corredores les gusta correr y por lo tanto corren')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'lo',\n",
       " 'corredor',\n",
       " 'le',\n",
       " 'gusta',\n",
       " 'correr',\n",
       " 'y',\n",
       " 'por',\n",
       " 'lo',\n",
       " 'tanto',\n",
       " 'corren']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('a los corredores les gusta correr y por lo tanto corren')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SOCIAL\n",
      "[nltk_data]     DATA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corredor', 'gusta', 'correr', 'corr']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "### probamos el \"tokenizer_porter\"\n",
    "stop = stopwords.words('Spanish')\n",
    "[w for w in tokenizer_porter('a un corredor le gusta correr y corre mucho')[-10:]\n",
    "if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado de temas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descomponer documentos de texto con LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation con scikit-learn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this movie was absolutely hilarious....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pluses: Mary Boland is delightfully on edge as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is not very bad tjough. But one can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I thought this movie was absolutely hilarious....          1\n",
       "1  Pluses: Mary Boland is delightfully on edge as...          0\n",
       "2  This movie is not very bad tjough. But one can...          0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, ## número de temas a conseguir, antes atributo n_topics\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "script worst action poor budget\n",
      "Topic 2:\n",
      "music dvd version original song\n",
      "Topic 3:\n",
      "guy stupid kids girl minutes\n",
      "Topic 4:\n",
      "war woman wife men death\n",
      "Topic 5:\n",
      "horror gore effects monster blood\n",
      "Topic 6:\n",
      "documentary true feel american family\n",
      "Topic 7:\n",
      "game series episode episodes cartoon\n",
      "Topic 8:\n",
      "role performance play plays actress\n",
      "Topic 9:\n",
      "action john plays role guy\n",
      "Topic 10:\n",
      "book comedy series read humor\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basándonos en la lectura de las 5 palabras más importantes para cada tema, podemos adivinar que el LDA identificó los siguientes temas:\n",
    "    \n",
    "1. Por lo general, películas malas (no es realmente una categoría de tema)\n",
    "2. Películas sobre familias\n",
    "3. Películas de guerra\n",
    "4. Películas de arte\n",
    "5. Películas de crimen\n",
    "6. Películas de terror\n",
    "7. Comedias\n",
    "8. Películas de alguna manera relacionadas con programas de televisión\n",
    "9. Películas basadas en libros\n",
    "10. Películas de acción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para confirmar que las categorías tienen sentido en función de las reseñas, vamos a trazar 5 películas de la categoría de películas de terror (categoría 6 en la posición de índice 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "\"Shadows\" is often acclaimed as the film that was the breakthrough for American independent cinema. Whether thats true or not, it is an undeniably important film, one whose influence can be traced all the way to today's Sundance fodder. Here is a film which tackles controversial topics of the day (n ...\n",
      "\n",
      "Horror movie #2:\n",
      "I won't go to a generalization, and say it's the best love story of all time, as some have said. That's fine, people feel very deeply about this film, you either love it I believe...or you simply hate it. I don't want to say, the best of all,because that is simply too 'broad' for me to make a statem ...\n",
      "\n",
      "Horror movie #3:\n",
      "In 2004, I wrote the following statements on an IMDb message board when a user wondered if The Best Years of Our Lives was a forgotten movie: <br /><br />***** To me watching this movie is like opening up a time capsule. I think in many ways \"The Best Years of Our Lives\" is probably one of the more  ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el ejemplo de código anterior, imprimimos los primeros 300 palabras de las 3 comentarios sobre películas de terror y, de hecho, podemos ver que las críticas - aunque no sabemos a qué película exacta pertenecen - suenan como reseñas de películas de terror, de hecho. (Sin embargo, se podría argumentar que las #2 de películas también podrían pertenecer a la categoría de tema 1.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
